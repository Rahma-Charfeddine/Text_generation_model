{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5e37c9d",
   "metadata": {},
   "source": [
    "# Text Generation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5eed41",
   "metadata": {},
   "source": [
    "Text Generation Models have various applications, such as content creation, chatbots, automated story writing, and more. They often utilize advanced Machine Learning techniques, particularly Deep Learning models like Recurrent Neural Networks (RNNs), Long Short-Term Memory Networks (LSTMs), and Transformer models like GPT (Generative Pre-trained Transformer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd18705",
   "metadata": {},
   "source": [
    "importing the necessary Python libraries and the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dcd6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373b6731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "# load the Tiny Shakespeare dataset\n",
    "dataset, info = tfds.load('tiny_shakespeare', with_info=True, as_supervised=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f714e304",
   "metadata": {},
   "source": [
    "converting the text to sequences of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cf2530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the text from the dataset\n",
    "text = next(iter(dataset['train']))['text'].numpy().decode('utf-8')\n",
    "\n",
    "# create a mapping from unique characters to indices\n",
    "vocab = sorted(set(text))\n",
    "char2idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "# numerically represent the characters\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "\n",
    "# create training examples and targets\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text) // (seq_length + 1)\n",
    "\n",
    "# create training sequences\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594fd6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88500f71",
   "metadata": {},
   "source": [
    "shuffling the dataset and packing it into training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba595af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25059252",
   "metadata": {},
   "source": [
    " Recurrent Neural Network model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4424fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of the vocabulary\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# the embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# number of RNN units\n",
    "rnn_units = 1024\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61a7c13",
   "metadata": {},
   "source": [
    "loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32c8e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f28fe23",
   "metadata": {},
   "source": [
    "training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69496ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "\n",
    "# name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True\n",
    ")\n",
    "\n",
    "# train the model\n",
    "EPOCHS = 10\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb818e3",
   "metadata": {},
   "source": [
    "rebuild the model with a batch size of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262f4b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9eafa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "generateing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674eebf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    num_generate = 1000\n",
    "\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    text_generated = []\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))\n",
    "\n",
    "print(generate_text(model, start_string=u\"QUEEN: So, lets end this\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a706606",
   "metadata": {},
   "source": [
    "the final output is a concatenation of the seed phrase with the newly generated characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b1879b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d124a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4484f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
